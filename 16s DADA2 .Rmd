---
title: "16S Processing pipeline -- Defaults"
subtitle: "Written by Gordon Custer"
output: html_notebook
---

# You can ignore this section unless you are curious about the individual steps in processing. 
This data is analyzed by the Dada2 pipeline and subsequent analysis in phyloseq and DeSeq2.

Dada2
https://benjjneb.github.io/dada2/tutorial.html
https://benjjneb.github.io/dada2/ITS_workflow.html

Phylseq
https://www.bioconductor.org/packages/devel/bioc/vignettes/phyloseq/inst/doc/phyloseq-analysis.html

DeSeq2 (Diffferential abundance analysis)
https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html

```{r}
library(dada2); packageVersion("dada2")
library(ShortRead)
```

Set pathway to files. 
```{r}
path <- "" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```

Split reads into forward and reverse reads. The pattern should match the end of each read. If individual files are unizpped remove the ".gz".
```{r}
fnFs <- sort(list.files(path, pattern = "*_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "*_R2_001.fastq.gz", full.names = TRUE))
```

Now we look for primers and remove them if we find any. Below are the 515f and 806r primers used in Custer et al. 2019. 
```{r}
FWD <- "GTGYCAGCMGCCGCGGTAA"  ## CHANGE ME to your forward primer sequence
REV <- "GGACTACNVGGGTWTCTAAT"  ## CHANGE ME...
```

Now, we take all orients of the primers and search for them. 
```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```

Make initial quality plots to get an idea of what the sample quality looks like.
```{r}
plotQualityProfile(fnFs[1:5])
plotQualityProfile(fnRs[1:5])
```

Initial filter step to trim off ends with bad quality. This is common with 2x300. This will reduces the number of reads you will lose later when tossing reads with Ns in them below. 
```{r}
fnFs.filt.1 <- file.path(path, "filt1", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filt.1 <- file.path(path, "filt1", basename(fnRs))
#these parameters should be changed based upon the output of the quality plots.
track.filt.1 <-filterAndTrim(fnFs, fnFs.filt.1, fnRs, fnRs.filt.1, truncQ = 10,  multithread = TRUE)
# look at the track.filt.1 object to see how many reads were lost in this step
track.filt.1
colSums(track.filt.1)
```

Plot reads again to see if they look better. 
```{r}
# plot the quality filters to see how they look
plotQualityProfile(fnFs.filt.1[1:5])
plotQualityProfile(fnRs.filt.1[1:5])
```

A second step for filtering which removes Reads with N's. The initial step was based soley on quality.
```{r}
fnFs.filt.2 <- file.path(path, "filt2", basename(fnFs)) # Put N-filterd files in filt2/ subdirectory
fnRs.filt.2 <- file.path(path, "filt2", basename(fnRs))
track.filt.2 <-filterAndTrim(fnFs.filt.1, fnFs.filt.2, fnRs.filt.1, fnRs.filt.2, maxN = 0, multithread = TRUE)
# look at the track.filt.2 object to see how many reads were lost in this step
track.filt.2
(cbind(track.filt.1, track.filt.2))
colSums(track.filt.2)
```

Now we count how many times the primers appear in our reads. The answer should be none. 
```{r}
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filt.2[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnFs.filt.2[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnRs.filt.2[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filt.2[[1]]))
```

If primers were found we would need to remove primers via cutadapt. You must install cutadapt first and show your machine where to find it.
```{r}
cutadapt <- "" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R
```

We now create output filenames for the cutadapt-ed files, and define the parameters we are going to give the cutadapt command. The critical parameters are the primers, and they need to be in the right orientation, i.e. the FWD primer should have been matching the forward-reads in its forward orientation, and the REV primer should have been matching the reverse-reads in its forward orientation. Warning: A lot of output will be written to the screen by cutadapt!
```{r}
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}
```

Now we check to make sure there are no primers found in our reads. If none are found we can move forward. 
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```

Check quality of samples again by plotting.
```{r}
plotQualityProfile(fnFs.filt.2[1:5])
plotQualityProfile(fnRs.filt.2[1:5])
```

The primer-free sequence files are now ready to be analyzed through the DADA2 pipeline. Similar to the earlier steps of reading in FASTQ files, we read in the names of the cutadapt-ed FASTQ files and applying some string manipulation to get the matched lists of forward and reverse fastq files.
```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path, pattern = "R1", full.names = TRUE))
cutRs <- sort(list.files(path, pattern = "R2", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```

Check quality of samples again by plotting.
```{r}
plotQualityProfile(cutFs[1:5])
plotQualityProfile(cutRs[1:5])
```

Assigning the filenames for the output of the filtered reads to be stored as fastq.gz files.
```{r}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

Next, we filter and trim based on the read characteristics including expected errors, minimum length and quality score.
```{r}
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(1, 1), 
    truncQ = 11, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out)
```

Next, learn errors and dereplication followed by the DADA step in which sequencing errors are removed and the undelying proportions of true biological replicates are tallied. 
```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)
```

Next, we merge the forward and reverse reads. This step can be further customized based on quality and the nubmer of reads retained through merging. Follwoing merging, we make a sequence table (OTU table) for downstream analsysis. 
```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

Next, we remove chimeric sequences from our sequence table.
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
#provides distriution of retained read lengths
table(nchar(getSequences(seqtab.nochim)))
```

This step provides a summary table of the number of reads retained per sample through processing. 
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, 
    getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", 
    "nonchim")
rownames(track) <- sample.names
head(track)
colSums(track)
```

Assign taxonomy based with desired database. 
```{r}
tax.ref <- ""  # CHANGE ME to location on your machine
taxa <- assignTaxonomy(seqtab.nochim, tax.ref, multithread = TRUE, tryRC = TRUE)
```

